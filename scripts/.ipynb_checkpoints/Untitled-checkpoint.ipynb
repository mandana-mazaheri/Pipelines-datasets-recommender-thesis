{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-liabilities",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fetchPipelinesInfo_v4():\n",
    "    ############################vvvvvvvvvvvvvvvvvvvvvv This section is from : /conp-portal/app/threads.py \n",
    "    searcher = Searcher(query=None, max_results=100, no_trunc=True, verbose=True)\n",
    "    all_descriptors = searcher.search()\n",
    "    # then pull every single descriptor\n",
    "    all_descriptor_ids = list(map(lambda x: x[\"ID\"], all_descriptors))\n",
    "    files = Puller(all_descriptor_ids).pull()\n",
    "\n",
    "    # fetch every single descriptor into one file\n",
    "    detailed_all_descriptors = list(map(lambda f: json.load(open(f, 'r')), files))\n",
    "    \n",
    "    # store data in cache\n",
    "    with open(pipelineDescriptors_FN, \"w\") as f:\n",
    "        json.dump(all_descriptors, f, indent=4)\n",
    "\n",
    "    #with open(detailed_all_descriptors_File, \"w\") as f:\n",
    "    #    json.dump(detailed_all_descriptors, f, indent=4)\n",
    "    ############################^^^^^^^^^^^^^^^^^^^^^^ This section is from : /conp-portal/app/threads.py \n",
    "    print(\"fetchPipelinesInfor_v4 Done\")\n",
    "    \n",
    "    \n",
    "def fetchDatasetsInfo_v4():\n",
    "    #Get information about datasets\n",
    "    RECOMMENDER_V4_DATASETS_PATH = os.path.join(os.getenv(\"HOME\"), \".cache/recommender-v4-datasets\")\n",
    "    if not os.path.exists(RECOMMENDER_V4_DATASETS_PATH):\n",
    "        os.mkdir(RECOMMENDER_V4_DATASETS_PATH)\n",
    "\n",
    "    repository = git.Git(RECOMMENDER_V4_DATASETS_PATH)\n",
    "    if not os.path.exists(RECOMMENDER_V4_DATASETS_PATH):\n",
    "        repository.clone(\"https://github.com/CONP-PCNO/conp-dataset.git\", recursive=True, branch='master', progress=True)\n",
    "    #else:\n",
    "    #    repository.checkout('master') #.fetch(\"https://github.com/CONP-PCNO/conp-dataset.git\", recursive=True, branch='master', progress=True)\n",
    "    #    repository.fetch()\n",
    "    print(\"fetchDatasetsInfo_v4 Done\")\n",
    "\n",
    "\n",
    "def getPipelinesAndDatasetsStatisctics_v4():\n",
    "\n",
    "    \n",
    "    with open(asessedPipelinesFile) as f:\n",
    "        asessedPipelines = list()\n",
    "        for item in list(f.read().split('\\n')):\n",
    "            if item.strip():\n",
    "                asessedPipelines += [item.strip()]\n",
    "\n",
    "    with open(pipelineDescriptors_FN) as f:\n",
    "        all_descriptors = json.load(f)\n",
    "    \n",
    "    #Get statistics for TAGS\n",
    "    pipelineDomains = []\n",
    "    for pipelineDescriptor in all_descriptors:\n",
    "        if pipelineDescriptor[\"TITLE\"] in asessedPipelines:\n",
    "            for tag in pipelineDescriptor['TAGS'].split(','):\n",
    "                if 'domain' in tag:\n",
    "                    tagName = tag.split(':')[1]\n",
    "                    if tagName not in pipelineDomains:\n",
    "                        pipelineDomains += [tagName]\n",
    "    \n",
    "    #Stores statisctics\n",
    "    statistic = {\n",
    "        \"Pipelines TAGS\": {}, \n",
    "        \"Pipelines CONTAINER\":{\"docker\" : 0, \"singularity\": 0}, \n",
    "        \"Datasets Format\": {}, \n",
    "        \"Datasets Modality\": {}, \n",
    "        \"Datasets Keyword\": {}, \n",
    "        \"Datasets File Size\": [],\n",
    "        \"Datasets Total Size (GB)\": 0,\n",
    "        \"Datasets Num Of Files\": 0,\n",
    "        \"Datasets Subjects\": 0,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for pipelineDomain in pipelineDomains:\n",
    "        statistic[\"Pipelines TAGS\"][pipelineDomain] = 0\n",
    "        \n",
    "    for pipelineTitle in asessedPipelines:\n",
    "        for pipelineDescriptor in all_descriptors:\n",
    "            if pipelineDescriptor[\"ID\"] != \"zenodo.3879740\":  #there are 2 \"TITLE\": \"BEst\"\n",
    "                if pipelineDescriptor[\"TITLE\"] == pipelineTitle:\n",
    "                    for pipelineDomain in pipelineDomains:\n",
    "                        if pipelineDomain in pipelineDescriptor['TAGS']:\n",
    "                            statistic[\"Pipelines TAGS\"][pipelineDomain] += 1\n",
    "\n",
    "                    #Get statistics for CONTAINER\n",
    "                    if \"docker\" in pipelineDescriptor[\"CONTAINER\"]:\n",
    "                        statistic[\"Pipelines CONTAINER\"][\"docker\"] += 1\n",
    "                    if \"singularity\" in pipelineDescriptor[\"CONTAINER\"]:\n",
    "                        statistic[\"Pipelines CONTAINER\"][\"singularity\"] += 1\n",
    "            \n",
    "    \n",
    "    RECOMMENDER_V4_DATASETS_PATH = os.path.join(os.getenv(\"HOME\"), \".cache/recommender-v4-datasets\")\n",
    "\n",
    "    crowlerFolderAddressList = os.listdir(os.path.join(RECOMMENDER_V4_DATASETS_PATH, \"conp-dataset\", \"projects\"))\n",
    "    for index, item in enumerate(crowlerFolderAddressList):\n",
    "        crowlerFolderAddressList[index] = os.path.join(RECOMMENDER_V4_DATASETS_PATH, \"conp-dataset\", \"projects\", item)\n",
    "\n",
    "    datasetDescriptorFileAddress = []\n",
    "    for folder in crowlerFolderAddressList:\n",
    "        if os.path.isdir(folder):\n",
    "            if os.path.exists(os.path.join(folder, \"DATS.json\")):\n",
    "                datasetDescriptorFileAddress += [os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder, \"DATS.json\")]\n",
    "            else:\n",
    "                subfolderList = os.listdir(os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder))\n",
    "                for index, item in enumerate(subfolderList):\n",
    "                    subfolderList[index] = os.path.join(RECOMMENDER_V4_DATASETS_PATH, folder, item)\n",
    "                crowlerFolderAddressList += subfolderList\n",
    "\n",
    "    allDatasetDescriptors = []\n",
    "    for datasetDescriptor in datasetDescriptorFileAddress:\n",
    "        with open(datasetDescriptor) as f:\n",
    "            allDatasetDescriptors += [json.load(f)]\n",
    "    \n",
    "    datasetFormats = []\n",
    "    datasetModalities = []\n",
    "    datasetKeywords = []\n",
    "    for datasetDescriptor in allDatasetDescriptors:\n",
    "        if \"formats\" in datasetDescriptor[\"distributions\"][0].keys():\n",
    "            for datasetFormat in datasetDescriptor[\"distributions\"][0][\"formats\"]:\n",
    "                if datasetFormat not in datasetFormats:\n",
    "                    datasetFormats += [datasetFormat]\n",
    "    \n",
    "    for datasetDescriptor in allDatasetDescriptors:\n",
    "        for t in datasetDescriptor.get('types', []):\n",
    "            info = t.get('information', {})\n",
    "            modality = info.get('value', None)\n",
    "            if (modality is not None) and (modality.lower() not in datasetModalities):\n",
    "                datasetModalities.append(modality.lower())\n",
    "        for t in datasetDescriptor.get('keywords', []):\n",
    "            keyword = t.get('value', None)\n",
    "            if (keyword is not None) and (keyword not in datasetKeywords):\n",
    "                datasetKeywords.append(keyword.lower())\n",
    "        \n",
    "    for datasetFormat in datasetFormats:\n",
    "        statistic[\"Datasets Format\"][datasetFormat] = 0\n",
    "    for datasetModality in datasetModalities:\n",
    "        statistic[\"Datasets Modality\"][datasetModality] = 0\n",
    "    for datasetKeyword in datasetKeywords:\n",
    "        statistic[\"Datasets Keyword\"][datasetKeyword] = 0\n",
    "\n",
    "    with open(assessedDatasetFile) as f:\n",
    "        assessedDatasets = f.readlines()\n",
    "    \n",
    "    statistic[\"Datasets Total Size (GB)\"] = 0\n",
    "    for datasetTitle in assessedDatasets:\n",
    "        for datasetDescriptorFile in datasetDescriptorFileAddress:\n",
    "            with open(datasetDescriptorFile) as f:\n",
    "                datasetDescriptor = json.load(f)\n",
    "                \n",
    "                if len(datasetTitle.strip().split(os.path.sep)) == 1:\n",
    "                    datasetTitle = datasetTitle.strip()\n",
    "                else:\n",
    "                    datasetTitle = datasetTitle.strip().split(os.path.sep)[1]\n",
    "                \n",
    "                if datasetTitle in datasetDescriptorFile.split(os.path.sep):\n",
    "                    if \"formats\" in datasetDescriptor[\"distributions\"][0].keys():\n",
    "                        for datasetFormat in datasetDescriptor[\"distributions\"][0][\"formats\"]:\n",
    "                            statistic[\"Datasets Format\"][datasetFormat] += 1\n",
    "\n",
    "                    for t in datasetDescriptor.get('types', []):\n",
    "                        info = t.get('information', {})\n",
    "                        modality = info.get('value', None)\n",
    "                        if (modality is not None):\n",
    "                            statistic[\"Datasets Modality\"][modality.lower()] += 1\n",
    "                    \n",
    "                    for t in datasetDescriptor.get('keywords', []):\n",
    "                        keyword = t.get('value', {})\n",
    "                        if (keyword is not None):\n",
    "                            statistic[\"Datasets Keyword\"][keyword.lower()] += 1\n",
    "    \n",
    "    \n",
    "                    dists = datasetDescriptor.get('distributions', None)\n",
    "                    if dists:\n",
    "                        if not type(dists) == list:\n",
    "                            if dists.get('@type', '') == 'DatasetDistribution':\n",
    "                                dist = dists\n",
    "                            else:\n",
    "                                dist = {}\n",
    "                        else:\n",
    "                            # Taking the first distribution size. (arbitrary choice)\n",
    "                            dist = dists[0]\n",
    "\n",
    "                        size = float(dist.get('size', 0))\n",
    "                        unit = dist.get('unit', {}).get('value', '')\n",
    "\n",
    "                        # Some data values from the DATS are not user friendly so\n",
    "                        # If size > 1000, divide n times until it is < 1000 and increment the units from the array\n",
    "                        units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB']\n",
    "                        count = 0\n",
    "\n",
    "                        while size > 1000:\n",
    "                            size /= 1000\n",
    "                            count += 1\n",
    "\n",
    "                        size = round(size, 1)\n",
    "                        unit = units[units.index(unit) + count]\n",
    "                        statistic[\"Datasets File Size\"] += [\"{} {}\".format(size, unit)]\n",
    "                        \n",
    "                        if unit == 'B':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000000000\n",
    "                        elif unit == 'KB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000000\n",
    "                        elif unit == 'MB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size / 1000\n",
    "                        elif unit == 'GB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size\n",
    "                        elif unit == 'TB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000\n",
    "                        elif unit == 'PB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000000\n",
    "                        elif unit == 'EB':\n",
    "                            statistic[\"Datasets Total Size (GB)\"] += size * 1000000000\n",
    "\n",
    "                    count = 0\n",
    "                    extraprops = datasetDescriptor.get('extraProperties', {})\n",
    "                    for prop in extraprops:\n",
    "                        if prop.get('category') == 'files':\n",
    "                            for x in prop.get('values', []):\n",
    "                                if isinstance(x['value'], str):\n",
    "                                    count += int(x['value'].replace(\",\", \"\"))\n",
    "                                else:\n",
    "                                    count += x['value']\n",
    "                    statistic[\"Datasets Num Of Files\"] += count\n",
    "\n",
    "                    count = 0\n",
    "                    extraprops = datasetDescriptor.get('extraProperties', {})\n",
    "                    for prop in extraprops:\n",
    "                        if prop.get('category') == 'subjects':\n",
    "                            for x in prop.get('values', []):\n",
    "                                if isinstance(x['value'], str):\n",
    "                                    count += int(x['value'].replace(\",\", \"\"))\n",
    "                                else:\n",
    "                                    count += x['value']\n",
    "                    statistic[\"Datasets Subjects\"] += count\n",
    "     \n",
    "    #remove \"\" from datasets keywords\n",
    "    if 'canadian-open-neuroscience-platform' in statistic[\"Datasets Keyword\"].keys():\n",
    "        del statistic[\"Datasets Keyword\"]['canadian-open-neuroscience-platform']\n",
    "    \n",
    "    \n",
    "    #Pipelines TAGS\n",
    "    for item in list(statistic[\"Pipelines TAGS\"]):\n",
    "        if statistic[\"Pipelines TAGS\"][item] == 0:\n",
    "            statistic[\"Pipelines TAGS\"].pop(item)\n",
    "            \n",
    "    sortedStatistic = {k: v for k, v in sorted(statistic[\"Pipelines TAGS\"].items(), key=lambda item: item[1], reverse=False)}\n",
    "    \n",
    "    y_pos = np.arange(len(sortedStatistic.keys()))\n",
    "    plt.subplots(figsize=(5,8))\n",
    "    plt.barh(y_pos, sortedStatistic.values(), align='center', alpha=1, color='red') \n",
    "    plt.margins(y=0.01)\n",
    "    plt.yticks(y_pos, sortedStatistic.keys(), fontsize=18) \n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.xlabel('Number of Pipelines', fontsize=18)\n",
    "    #plt.ylabel('Pipelines Tag', fontsize=18)\n",
    "    #plt.savefig(os.path.join(figuresFolder, 'Pipelines Tag.jpg'), bbox_inches=\"tight\")\n",
    "    plt.savefig(os.path.join(figuresFolder, 'Pipelines Tag.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    #Datasets Type\n",
    "    for item in list(statistic[\"Datasets Keyword\"]):\n",
    "        if statistic[\"Datasets Keyword\"][item] == 0:\n",
    "            statistic[\"Datasets Keyword\"].pop(item)\n",
    "    \n",
    "    sortedStatistic = {k: v for k, v in sorted(statistic[\"Datasets Keyword\"].items(), key=lambda item: item[1], reverse=False)}\n",
    "    \n",
    "    y_pos = np.arange(len(sortedStatistic.values()))\n",
    "    plt.subplots(figsize=(5,30))\n",
    "    plt.barh(y_pos, sortedStatistic.values(), align='center', alpha=1, color='blue') \n",
    "    plt.margins(y=0.01)\n",
    "    plt.yticks(y_pos, sortedStatistic.keys(), fontsize=18)  \n",
    "    xValues = [*range(0, max(sortedStatistic.values()) + 1, 1)]\n",
    "    plt.xticks(xValues, xValues, fontsize=18)\n",
    "    plt.xlabel('Number of Datasets', fontsize=18)\n",
    "    #plt.ylabel('Dataset Keyword', fontsize=18)\n",
    "    #plt.savefig(os.path.join(figuresFolder, 'Datasets Keyword.jpg'), bbox_inches=\"tight\")\n",
    "    plt.savefig(os.path.join(figuresFolder, 'Datasets Keyword.pdf'), bbox_inches=\"tight\")\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    saveReportFileJson(\"Pipelines and Datasets Statistics.json\", statistic)\n",
    "    print(\"getPipelinesAndDatasetsStatisctics_v4 Done\")\n",
    "    \n",
    "fetchPipelinesInfo_v4()            \n",
    "fetchDatasetsInfo_v4()            \n",
    "getPipelinesAndDatasetsStatisctics_v4()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
