\section{Memory Manager}

The Memory Manager simulates two parallel threads: the main one
implements flushing, eviction, and cached I/Os synchronously, whereas
the second one, which operates in the background, periodically searches for
expired dirty data in LRU lists and flushes this data to disk. We
use existing storage simulation models~\cite{lebre2015} to simulate disk and
memory, characterized by their storage capacity, read and write
bandwidths, and latency. These models account for
bandwidth sharing between concurrent memory or disk accesses.

\subsection{Page cache LRU lists}

In the Linux kernel, page cache LRU lists contain file pages. However,
due to the large number of file pages, simulating lists of pages
induces substantial overhead.
Therefore, we introduce the concept of a data block as a unit to represent data
cached in memory. A data block is a subset of file pages stored in
page cache that were accessed in the same I/O operation.
A data block stores the file name, block size, last access
time, a dirty flag that represents whether the data is clean (0)
or dirty (1), and an entry (creation) time.
Blocks can have different sizes and a given file can have multiple
data blocks in page cache. In addition, a data block can be split into an
arbitrary number of smaller blocks.

\begin{figure}
       \centering
       \includegraphics[width=0.8\columnwidth]{figures/lru_lists.pdf}
       \caption{Model of page cache LRU lists with data blocks.}
       \label{fig:lrulist}
\end{figure}

We model page cache LRU lists as
two lists of data blocks, an active list and an inactive list, both ordered by
last access time (earliest first, Figure~\ref{fig:lrulist}).
As in the kernel, our simulator limits the size of the active list to
twice the size of the inactive list, by moving least recently
used data blocks from the active list to the inactive list~\cite{gorman2004understanding, linuxdev3rd2010}.

At any given time, a file can be partially cached, completely cached,
or not cached at all. A cached data block can only reside in one of two
LRU lists. The first time they are accessed, blocks are
added to the inactive list. On subsequent accesses, blocks of the
inactive list are moved to the top of the active list. Blocks
written to cache are marked dirty until flushed.

\subsection{Reads and writes}

Our simulation model supports chunk-by-chunk file accesses
with a user-defined chunk size. However, for simplicity, we assume that file pages are
accessed in a round-robin fashion rather than fully randomly.
Therefore, when a file is read, cached data is read only after all uncached data was read, and data from the inactive list is read
before data from the active list
(data reads occur from left to right in Figure~\ref{fig:read_order}).
When a chunk of \emph{uncached} data is read, a new clean block is created
and appended to the inactive list.
When a chunk of \emph{cached} data is read, one or more existing data blocks in the LRU lists are accessed.
If these blocks are clean, we merge them together, update the access time and size of the resulting block,
and append it to the active list.
If the blocks are dirty, we move them independently to the active list, to preserve their entry time.
Because the chunk and block sizes may be different, there are situations
where a block is not entirely read.
In this case, the block is split in two smaller blocks and one of them is re-accessed.
% From Tristan: not sure where to put this nor if it's necessary:
% , in which chunks are read/written
% until file is entirely read/written.

\begin{figure}
       \centering
       \includegraphics[width=0.8\columnwidth]{figures/read_order.pdf}
       \caption{File data read order. Data is read from left to right: uncached data
       is read first, followed by data from the inactive list, and finally data from the active list. }
       \label{fig:read_order}
\end{figure}

For file writes, we assume that all data to be written is
uncached. Thus, each time a chunk is written, we create a block of dirty data
and append it to the inactive list.

\subsection{Flushing and eviction}

The main simulated thread in the Memory Manager can flush or evict data from the
memory cache. The data flushing simulation
function takes the amount of data to flush as parameter. While
this amount is not reached and dirty
blocks remain in cache, this function traverses the sorted
inactive list, then the sorted active list, and writes the
least recently used dirty block to disk, having set its dirty
flag to 0. In case the amount of data to flush requires that a
block be partially flushed, the block is split in two blocks,
one that is flushed and one that remains dirty. The time needed
to flush data to disk is simulated by the storage model.

The cache eviction simulation also runs in
the main thread. It frees up the page cache by traversing and deleting
least recently used clean data blocks in the inactive list.
The amount of data to evict is passed as a parameter and data blocks are deleted
from the inactive list until the evicted data reaches the required amount,
or until there is no clean block left in the list.
If the last evicted block does not have to be entirely evicted, the block is split in two blocks,
and only one of them is evicted.
The overhead of the cache eviction algorithm is not part of the simulated time
since cache eviction time is negligible in real systems. % \tristan{a reference would be welcome}.

\begin{algorithm}[H]
\caption{Periodical flush simulation in Memory Manager}
\label{alg:pdflush}
    \small
    \begin{algorithmic}[1]
        \Input
            \Desc{in}{page cache inactive list}
            \Desc{ac}{page cache active list}
            \Desc{t}{predefined flushing time interval}
            \Desc{exp}{predefined expiration time}
            \Desc{sm}{storage simulation model}
           \EndInput
           \While{host is on}
            \State blocks = expired\_blocks(exp, in) + expired\_blocks(exp, ac)
            \State flushing\_time = 0
            \For{blk in blocks} 
              \State blk.dirty = 0 
              \State flushing\_time = flushing\_time + sm.write(blocks)
            \EndFor
            \If{flushing\_time $<$ t}
                \State sleep(t - flushing\_time)
            \EndIf  
        \EndWhile
    \end{algorithmic}
\end{algorithm}

Periodical flushing is simulated in the Memory Manager
background thread. As in the Linux kernel, a dirty block
in our model is considered expired if
the duration since its entry time is longer than a
predefined expiration time.
Periodical flushing is simulated as an infinite loop in which
the Memory Manager searches for dirty blocks and flushes them to disk (Algorithm~\ref{alg:pdflush}).
% From Tristan: the algorithm is quite straightforward, I don't think this is
% necessary
% In each repetition, Memory Manager finds expired dirty blocks in two
% page cache LRU lists (line 8), simulates writes of data of these blocks
% to disk (line 9), and mark them as clean (line 10).
% If the flushing time does not exceed our time interval, the thread is put
% to sleep for the remaining time (lines 11-13).
% Then based on the state of the host at current simulated time,
% the algorithm continues or finishes the loop.
Because periodical flushing is simulated as a background thread, it can happen concurrently
with disk I/O initiated by the main thread. This is taken into account by the
storage model and reflected in simulated I/O time.